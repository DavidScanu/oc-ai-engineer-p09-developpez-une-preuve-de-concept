import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import numpy as np
import sys
import os
from utils.model_manager import ModelManager, format_improvement, get_performance_status, DISTILBERT_BASELINE

# Ajouter le chemin parent pour les imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.model_manager import ModelManager, format_improvement, get_performance_status

st.set_page_config(
    page_title="M√©triques des Mod√®les",
    page_icon="üìà",
    layout="wide"
)

@st.cache_data
def load_model_manager():
    """Cache le gestionnaire de mod√®les"""
    return ModelManager()

def create_comparison_chart(comparison_df):
    """Cr√©e un graphique de comparaison multi-mod√®les"""
    # Pr√©parer les donn√©es pour le graphique
    metrics = ['Accuracy', 'F1-Score', 'Pr√©cision', 'Rappel', 'ROC AUC']
    
    fig = go.Figure()
    
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
    
    for i, (_, row) in enumerate(comparison_df.iterrows()):
        model_name = row['Mod√®le']
        color = colors[i % len(colors)]
        
        # Couleur sp√©ciale pour le baseline
        if row['Type'] == 'baseline':
            color = '#ff7f0e'
            model_name += ' (Baseline)'
        
        fig.add_trace(go.Scatter(
            x=metrics,
            y=[row[metric] for metric in metrics],
            mode='lines+markers',
            name=model_name,
            line=dict(color=color, width=3),
            marker=dict(size=8),
            text=[f'{row[metric]:.3f}' for metric in metrics],
            textposition='top center'
        ))
    
    fig.update_layout(
        title='Comparaison des Performances des Mod√®les',
        title_font_size=18,
        xaxis_title='M√©triques',
        yaxis_title='Score',
        yaxis=dict(range=[0.7, 1.0]),
        plot_bgcolor='rgba(0,0,0,0)',
        font=dict(size=12),
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="center",
            x=0.5
        ),
        height=500
    )
    
    return fig

def create_improvement_chart(comparison):
    """Cr√©e un graphique des am√©liorations"""
    improvements = comparison['improvements']
    
    metrics = list(improvements.keys())
    values = [improvements[metric]['percentage'] for metric in metrics]
    colors = ['green' if v > 0 else 'red' for v in values]
    
    fig = go.Figure(data=[
        go.Bar(
            x=metrics,
            y=values,
            marker_color=colors,
            text=[f'{v:+.1f}%' for v in values],
            textposition='outside'
        )
    ])
    
    fig.add_hline(y=0, line_dash="dash", line_color="black")
    
    fig.update_layout(
        title='Am√©liorations vs Baseline DistilBERT',
        title_font_size=16,
        xaxis_title='M√©triques',
        yaxis_title='Am√©lioration (%)',
        plot_bgcolor='rgba(0,0,0,0)',
        font=dict(size=12)
    )
    
    return fig

def create_model_comparison_chart(selected_model, baseline):
    """Cr√©e un graphique de comparaison avec le baseline (style barres group√©es)"""
    
    # Pr√©parer les donn√©es
    models_data = [
        {
            'Mod√®le': 'DistilBERT (Baseline)',
            'Accuracy': baseline['accuracy'],
            'F1-score': baseline['f1'],
            'Precision': baseline['precision'],
            'Recall': baseline['recall'],
            'ROC AUC': baseline['roc_auc']
        },
        {
            'Mod√®le': f'ModernBERT ({selected_model["model_id"]})',
            'Accuracy': selected_model['accuracy'],
            'F1-score': selected_model['f1'],
            'Precision': selected_model['precision'],
            'Recall': selected_model['recall'],
            'ROC AUC': selected_model['roc_auc']
        }
    ]
    
    df = pd.DataFrame(models_data)
    
    # Graphique en barres group√©es
    fig = go.Figure()
    
    colors = ['#ff7f0e', '#2ca02c']  # Orange et vert
    metrics = ['Accuracy', 'F1-score', 'Precision', 'Recall', 'ROC AUC']
    
    for i, model in enumerate(df['Mod√®le']):
        fig.add_trace(go.Bar(
            name=model,
            x=metrics,
            y=[df.iloc[i][metric] for metric in metrics],
            marker_color=colors[i],
            text=[f'{df.iloc[i][metric]:.3f}' for metric in metrics],
            textposition='auto'
        ))
    
    fig.update_layout(
        title='Comparaison des Performances des Mod√®les',
        title_font_size=18,
        xaxis_title='M√©triques',
        yaxis_title='Score',
        yaxis=dict(range=[0.75, 1.0]),
        barmode='group',
        plot_bgcolor='rgba(0,0,0,0)',
        font=dict(size=12),
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        )
    )
    
    return fig, df


def main():
    st.title("üìà M√©triques et Comparaison des Mod√®les")
    st.markdown("---")
    
    # Initialiser le gestionnaire de mod√®les
    manager = load_model_manager()
    
    # D√©couvrir les mod√®les disponibles
    available_models = manager.discover_models()
    
    st.subheader("üîç Mod√®les Disponibles")
    
    if not available_models:
        st.warning("‚ö†Ô∏è Aucun mod√®le ModernBERT trouv√© dans le dossier models/")
        st.info("üí° Ajoutez un mod√®le dans le format : `models/modernbert-sentiment-yyyymmdd_hhmm/`")
        st.stop()
    
    # S√©lecteur de mod√®le
    col1, col2 = st.columns([2, 1])
    
    with col1:
        model_options = {f"{model['model_id']} ({model['training_date']})": model for model in available_models}
        selected_model_key = st.selectbox(
            "Choisissez un mod√®le √† analyser :",
            options=list(model_options.keys()),
            index=0
        )
        selected_model = model_options[selected_model_key]
    
    with col2:
        st.metric("Mod√®les disponibles", len(available_models))
        
        # Meilleur mod√®le
        best_model = manager.get_best_model('roc_auc')
        if best_model:
            is_best = best_model['model_id'] == selected_model['model_id']
            st.metric(
                "Statut", 
                "‚úîÔ∏è Meilleur" if is_best else "üìä Standard",
                help="Bas√© sur le ROC AUC"
            )
    
    # Comparaison avec baseline
    comparison = manager.compare_with_baseline(selected_model)
    status_color, status_message = get_performance_status(comparison)
    
    # Affichage du statut
    if status_color == "success":
        st.success(status_message)
    elif status_color == "info":
        st.info(status_message)
    elif status_color == "warning":
        st.warning(status_message)
    else:
        st.error(status_message)
    
    st.markdown("---")
    
    # M√©triques d√©taill√©es
    st.subheader(f"üìä Performances D√©taill√©es - {selected_model['model_id']}")
    
    col1, col2, col3, col4, col5 = st.columns(5)
    
    metrics_info = [
        ("Accuracy", "accuracy", "Pr√©cision globale"),
        ("F1-Score", "f1", "Moyenne harmonique"),
        ("Pr√©cision", "precision", "Vrais positifs"),
        ("Rappel", "recall", "Sensibilit√©"),
        ("ROC AUC", "roc_auc", "Discrimination")
    ]
    
    for i, (label, key, help_text) in enumerate(metrics_info):
        with [col1, col2, col3, col4, col5][i]:
            model_value = selected_model.get(key, 0)
            improvement = comparison['improvements'].get(key, {})
            delta_value = improvement.get('absolute', 0)
            
            st.metric(
                label,
                f"{model_value:.3f}",
                delta=f"{delta_value:+.3f}",
                delta_color="normal" if delta_value >= 0 else "inverse",
                help=help_text
            )


    # Informations techniques
    st.markdown("---")
    st.subheader("üîß Informations Techniques")
    
    # Premi√®re ligne - Mod√®le
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Architecture", selected_model.get('architecture', 'N/A'))
    with col2:
        training_time = selected_model.get('training_time_minutes', 0)
        st.metric("Temps d'entra√Ænement", f"{training_time:.1f} min")
    with col3:
        total_params = selected_model.get('total_params', 0)
        st.metric("Param√®tres totaux", f"{total_params:,}")
    with col4:
        trainable_params = selected_model.get('trainable_params', 0)
        st.metric("Param√®tres entra√Ænables", f"{trainable_params:,}")
    
    # Deuxi√®me ligne - Dataset et entra√Ænement
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        epochs = selected_model.get('epochs_completed', 'N/A')
        st.metric("√âpoques", f"{epochs}")
    with col2:
        # Taille totale du dataset
        dataset_size = selected_model.get('total_samples', 0)
        if dataset_size > 0:
            st.metric("Dataset total", f"{dataset_size:,}")
        else:
            st.metric("Dataset total", "N/A")
    with col3:
        # √âchantillons d'entra√Ænement
        train_samples = selected_model.get('train_samples', 0)
        if train_samples > 0:
            st.metric("√âchantillons train", f"{train_samples:,}")
        else:
            st.metric("√âchantillons train", "N/A")
    with col4:
        # √âquilibre des classes
        positive_samples = selected_model.get('positive_samples', 0)
        negative_samples = selected_model.get('negative_samples', 0)
        
        if positive_samples > 0 and negative_samples > 0:
            balance_ratio = positive_samples / (positive_samples + negative_samples) * 100
            st.metric("√âquilibre pos/neg", f"{balance_ratio:.0f}%/{100-balance_ratio:.0f}%")
        else:
            st.metric("√âquilibre pos/neg", "N/A")



    # Graphiques de comparaison
    st.markdown("---")
    st.subheader("üîÑ Comparaison avec le Baseline")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        fig_comparison, df_comparison = create_model_comparison_chart(selected_model, manager.baseline)
        st.plotly_chart(fig_comparison, use_container_width=True)
    
    with col2:
        st.markdown("**Am√©liorations vs DistilBERT :**")
        
        improvements = comparison['improvements']
        
        for metric, improvement_data in improvements.items():
            improvement_pct = improvement_data['percentage']
            st.write(f"‚Ä¢ **{metric.title()}**: {improvement_pct:+.1f}%")
        
        # Message conditionnel selon les performances
        if comparison['summary']['is_better']:
            st.success("‚úÖ Toutes les m√©triques sont am√©lior√©es !")
        elif comparison['summary']['metrics_improved'] > 0:
            st.warning(f"‚ö†Ô∏è {comparison['summary']['metrics_improved']}/{comparison['summary']['total_metrics']} m√©triques am√©lior√©es")
        else:
            st.error("‚ùå Aucune m√©trique am√©lior√©e vs baseline")
    
    # Graphique des am√©liorations individuelles
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìà D√©tail des Am√©liorations")
        fig_improvements = create_improvement_chart(comparison)
        st.plotly_chart(fig_improvements, use_container_width=True)
        
        # R√©sum√© des am√©liorations
        summary = comparison['summary']
        st.write(f"**Am√©lioration moyenne :** {summary['avg_improvement']:+.1f}%")
        st.write(f"**M√©triques am√©lior√©es :** {summary['metrics_improved']}/{summary['total_metrics']}")
    
    with col2:
        st.subheader("üìä Comparaison Multi-Mod√®les")
        
        # Inclure tous les mod√®les dans la comparaison
        comparison_df = manager.get_comparison_dataframe()
        fig_comparison_all = create_comparison_chart(comparison_df)
        st.plotly_chart(fig_comparison_all, use_container_width=True)


    # Tableau de comparaison d√©taill√©
    st.markdown("---")
    st.subheader("üìã Tableau de Comparaison Complet")
    
    # Formater le DataFrame pour l'affichage
    display_df = comparison_df.copy()
    
    # Arrondir les valeurs num√©riques
    numeric_columns = ['Accuracy', 'F1-Score', 'Pr√©cision', 'Rappel', 'ROC AUC']
    for col in numeric_columns:
        display_df[col] = display_df[col].round(3)
    
    # Styliser le tableau
    def highlight_best(s):
        """Surligne les meilleures valeurs"""
        if s.name in numeric_columns:
            max_val = s.max()
            return ['background-color: #e8f5e8' if v == max_val else '' for v in s]
        return [''] * len(s)
    
    def highlight_baseline(row):
        """Surligne la ligne baseline"""
        if row['Type'] == 'baseline':
            return ['background-color: #fff3cd'] * len(row)
        return [''] * len(row)
    
    styled_df = display_df.style.apply(highlight_best, axis=0).apply(highlight_baseline, axis=1)
    
    st.dataframe(styled_df, use_container_width=True)
    
    # Analyse d√©taill√©e
    with st.expander("üîç Analyse D√©taill√©e"):
        st.markdown(f"""
        **Mod√®le analys√© :** {selected_model['model_id']}
        
        **Points forts :**
        """)
        
        for metric, improvement in comparison['improvements'].items():
            if improvement['better']:
                st.write(f"‚Ä¢ **{metric.title()}** : {format_improvement(improvement)} vs baseline")
        
        st.markdown("**Points d'am√©lioration :**")
        
        for metric, improvement in comparison['improvements'].items():
            if not improvement['better']:
                st.write(f"‚Ä¢ **{metric.title()}** : {format_improvement(improvement)} vs baseline")
        
        # Recommandations
        st.markdown("**Recommandations :**")
        
        if comparison['summary']['is_better']:
            if comparison['summary']['significant_improvement']:
                st.write("üéØ **Mod√®le recommand√© pour la production**")
                st.write("‚Ä¢ Performance sup√©rieure au baseline sur la majorit√© des m√©triques")
                st.write("‚Ä¢ Am√©lioration significative d√©tect√©e")
            else:
                st.write("üìà **Mod√®le prometteur mais perfectible**")
                st.write("‚Ä¢ L√©g√®re am√©lioration vs baseline")
                st.write("‚Ä¢ Consid√©rer des optimisations suppl√©mentaires")
        else:
            st.write("‚ö†Ô∏è **Optimisation requise**")
            st.write("‚Ä¢ Performance inf√©rieure au baseline")
            st.write("‚Ä¢ R√©viser l'architecture ou les hyperparam√®tres")
            st.write("‚Ä¢ Analyser les causes de sous-performance")
    



    # R√©sum√© ex√©cutif dynamique
    st.markdown("---")
    st.subheader("üìã R√©sum√© Ex√©cutif")
    
    # Calculer les m√©triques cl√©s pour le r√©sum√©
    summary = comparison['summary']
    selected_metrics = {
        'accuracy': selected_model.get('accuracy', 0),
        'f1': selected_model.get('f1', 0),
        'precision': selected_model.get('precision', 0),
        'recall': selected_model.get('recall', 0),
        'roc_auc': selected_model.get('roc_auc', 0)
    }
    
    baseline_metrics = {
        'accuracy': manager.baseline['accuracy'],
        'f1': manager.baseline['f1'],
        'precision': manager.baseline['precision'],
        'recall': manager.baseline['recall'],
        'roc_auc': manager.baseline['roc_auc']
    }
    
    # Informations techniques
    training_time = selected_model.get('training_time_minutes', 0)
    total_params = selected_model.get('total_params', 0)
    trainable_params = selected_model.get('trainable_params', 0)
    trainable_ratio = (trainable_params / total_params * 100) if total_params > 0 else 0
    
    # Messages conditionnels selon les performances
    if summary['is_better'] and summary['significant_improvement']:
        # Mod√®le excellent
        st.success(f"""
        **üéØ MOD√àLE RECOMMAND√â POUR LA PRODUCTION**
        
        **Mod√®le :** {selected_model['model_id']} ({selected_model['training_date']})
        
        **Performance :**
        ‚úÖ **ROC AUC excellent** : {selected_metrics['roc_auc']:.1%} (+{comparison['improvements']['roc_auc']['percentage']:+.1f}% vs baseline)
        ‚úÖ **Accuracy sup√©rieure** : {selected_metrics['accuracy']:.1%} (+{comparison['improvements']['accuracy']['percentage']:+.1f}% vs baseline)
        ‚úÖ **F1-Score optimal** : {selected_metrics['f1']:.1%} (+{comparison['improvements']['f1']['percentage']:+.1f}% vs baseline)
        ‚úÖ **Am√©lioration moyenne** : {summary['avg_improvement']:+.1f}% sur toutes les m√©triques
        
        **Efficacit√© :**
        ‚ö° **Entra√Ænement efficace** : {training_time:.1f} minutes seulement
        üéØ **Param√®tres optimis√©s** : {trainable_params:,} param√®tres fine-tun√©s ({trainable_ratio:.3f}% du total)
        
        **Recommandation :** D√©ploiement imm√©diat recommand√©
        """)
        
    elif summary['is_better']:
        # Mod√®le avec am√©lioration mod√©r√©e
        st.info(f"""
        **üìà MOD√àLE PROMETTEUR - OPTIMISATION POSSIBLE**
        
        **Mod√®le :** {selected_model['model_id']} ({selected_model['training_date']})
        
        **Performance :**
        ‚úÖ **ROC AUC** : {selected_metrics['roc_auc']:.1%} (+{comparison['improvements']['roc_auc']['percentage']:+.1f}% vs baseline)
        ‚úÖ **Accuracy** : {selected_metrics['accuracy']:.1%} (+{comparison['improvements']['accuracy']['percentage']:+.1f}% vs baseline)
        ‚úÖ **M√©triques am√©lior√©es** : {summary['metrics_improved']}/{summary['total_metrics']} au-dessus du baseline
        üìä **Am√©lioration moyenne** : {summary['avg_improvement']:+.1f}%
        
        **Points forts :**
        """)
        
        # Lister les m√©triques am√©lior√©es
        for metric, improvement in comparison['improvements'].items():
            if improvement['better']:
                st.write(f"   ‚Ä¢ **{metric.title()}** : {improvement['percentage']:+.1f}% d'am√©lioration")
        
        st.info(f"""
        **Recommandation :** Consid√©rer pour production apr√®s tests A/B
        """)
        
    elif summary['avg_improvement'] > -2.0:
        # Performance similaire
        st.warning(f"""
        **‚ö†Ô∏è PERFORMANCE SIMILAIRE AU BASELINE**
        
        **Mod√®le :** {selected_model['model_id']} ({selected_model['training_date']})
        
        **Performance :**
        üìä **ROC AUC** : {selected_metrics['roc_auc']:.1%} ({comparison['improvements']['roc_auc']['percentage']:+.1f}% vs baseline)
        üìä **Accuracy** : {selected_metrics['accuracy']:.1%} ({comparison['improvements']['accuracy']['percentage']:+.1f}% vs baseline)
        ‚öñÔ∏è **Diff√©rence moyenne** : {summary['avg_improvement']:+.1f}% (non significative)
        
        **Analyse :**
        ‚Ä¢ Pas d'am√©lioration significative d√©tect√©e
        ‚Ä¢ Performance √©quivalente au baseline DistilBERT
        ‚Ä¢ Co√ªt computationnel plus √©lev√© sans b√©n√©fice clair
        
        **Recommandation :** Continuer l'optimisation ou rester sur DistilBERT
        """)
        
    else:
        # Performance inf√©rieure
        st.warning(f"""
        **üö® OPTIMISATION REQUISE - NON RECOMMAND√â POUR PRODUCTION**
        
        **Mod√®le :** {selected_model['model_id']} ({selected_model['training_date']})
        
        **Performance :**
        - ‚ùå **ROC AUC inf√©rieur** : {selected_metrics['roc_auc']:.1%} ({comparison['improvements']['roc_auc']['percentage']:+.1f}% vs baseline)
        - ‚ùå **Accuracy r√©duite** : {selected_metrics['accuracy']:.1%} ({comparison['improvements']['accuracy']['percentage']:+.1f}% vs baseline)
        - üìâ **D√©gradation moyenne** : {summary['avg_improvement']:+.1f}% sur les m√©triques
        
        **Probl√®mes identifi√©s :**
        - Sous-performance sur les m√©triques cl√©s
        - Pas d'am√©lioration significative par rapport au baseline DistilBERT
        - Ratio d'entra√Ænement faible : {trainable_ratio:.3f}% des param√®tres entra√Ænables

        **Causes possibles :**
        - Hyperparam√®tres inad√©quats (learning rate: trop √©lev√©/bas)
        - Gel excessif ({trainable_ratio:.3f}% param√®tres entra√Ænables)
        
        **Actions recommand√©es :**
        1. üîÑ R√©-entra√Ænement avec hyperparam√®tres optimis√©s
        2. üéØ D√©gel de plus de couches BERT
        3. üìà Augmentation du temps d'entra√Ænement
        4. üîç Analyse approfondie des erreurs de pr√©diction
        
        **D√©cision :** Rester sur DistilBERT jusqu'√† am√©lioration
        """)

        st.success("üöÄ **Prochaines √©tapes :** Optimisation et r√©-entra√Ænement du mod√®le")
    
    # Section technique d√©taill√©e
    with st.expander("üîß D√©tails Techniques du Mod√®le"):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("**Configuration :**")
            st.write(f"‚Ä¢ Architecture : {selected_model.get('architecture', 'N/A')}")
            st.write(f"‚Ä¢ Vocabulaire : {selected_model.get('vocab_size', 0):,} tokens")
            st.write(f"‚Ä¢ Param√®tres totaux : {total_params:,}")
            st.write(f"‚Ä¢ Param√®tres entra√Æn√©s : {trainable_params:,}")
            st.write(f"‚Ä¢ Ratio entra√Ænable : {trainable_ratio:.3f}%")
        
        with col2:
            st.markdown("**Entra√Ænement :**")
            st.write(f"‚Ä¢ Temps total : {training_time:.1f} minutes")
            st.write(f"‚Ä¢ √âpoques : {selected_model.get('epochs_completed', 'N/A')}")
            st.write(f"‚Ä¢ Steps totaux : {selected_model.get('total_steps', 'N/A'):,}")
            st.write(f"‚Ä¢ Loss finale : {selected_model.get('loss', 0):.4f}")
            st.write(f"‚Ä¢ Meilleure m√©trique : {selected_model.get('best_metric', 'N/A')}")
        
        with col3:
            st.markdown("**Comparaison Baseline :**")
            st.write(f"‚Ä¢ DistilBERT Accuracy : {baseline_metrics['accuracy']:.1%}")
            st.write(f"‚Ä¢ DistilBERT F1 : {baseline_metrics['f1']:.1%}")
            st.write(f"‚Ä¢ DistilBERT ROC AUC : {baseline_metrics['roc_auc']:.1%}")
            st.write(f"‚Ä¢ √âcart moyen : {summary['avg_improvement']:+.1f}%")
            st.write(f"‚Ä¢ M√©triques ‚úÖ : {summary['metrics_improved']}/{summary['total_metrics']}")
    
    # Recommandations d'am√©lioration si n√©cessaire
    if not summary['is_better'] or not summary['significant_improvement']:
        with st.expander("üí° Strat√©gies d'Am√©lioration"):
            st.markdown("""
            **Approches recommand√©es pour optimiser les performances :**
            
            ### üéØ Optimisation des Hyperparam√®tres
            ```python
            # Configuration A - D√©gel progressif
            learning_rates = {
                'classifier': 2e-5,
                'bert_layers': [1e-6, 5e-6, 1e-5, 2e-5]  # Graduel
            }
            
            # Configuration B - Learning rate adaptatif
            scheduler = 'cosine_with_restarts'
            warmup_ratio = 0.1
            ```
            
            ### üîÑ Strat√©gies d'Entra√Ænement
            - **D√©gel progressif** : Commencer par le classifier, puis d√©geler couche par couche
            - **Curriculum learning** : Entra√Æner d'abord sur exemples faciles
            - **Mixup/CutMix** : Augmentation de donn√©es pour robustesse
            
            ### üìä Analyse et Monitoring
            - **Validation curves** : Surveiller overfitting en temps r√©el
            - **Gradient analysis** : V√©rifier la propagation des gradients
            - **Layer-wise metrics** : Analyser l'apprentissage par couche
            
            ### üîç Diagnostic Avanc√©
            - **Error analysis** : Identifier les types d'erreurs r√©currentes
            - **Attention visualization** : Comprendre le focus du mod√®le
            - **Embedding analysis** : V√©rifier la qualit√© des repr√©sentations
            """)
    
    # Call-to-action selon le contexte
    st.markdown("---")
    
    if summary['is_better'] and summary['significant_improvement']:
        st.success("üöÄ **Pr√™t pour le d√©ploiement !** Ce mod√®le peut √™tre mis en production.")
        
        col1, col2 = st.columns(2)
        with col1:
            st.info("üìã **Prochaines √©tapes :**\n1. Tests d'int√©gration\n2. Validation m√©tier\n3. D√©ploiement progressif")
        with col2:
            st.info("üìä **Monitoring recommand√© :**\n1. Drift detection\n2. Performance tracking\n3. A/B testing continu")
    
    else:
        st.warning("üîÑ **Optimisation n√©cessaire** avant mise en production.")
        
        col1, col2 = st.columns(2)
        with col1:
            st.info("üéØ **Actions prioritaires :**\n1. R√©-entra√Ænement optimis√©\n2. Validation crois√©e\n3. Analyse d'erreurs")
        with col2:
            st.info("‚è±Ô∏è **Timeline sugg√©r√©e :**\n1. Optimisation : 1-2 semaines\n2. Tests : 1 semaine\n3. Validation : 1 semaine")








    # Export des donn√©es
    st.markdown("---")
    
    with st.expander("üíæ Export des Donn√©es"):
        st.markdown("**Donn√©es de comparaison au format JSON :**")
        
        export_data = {
            'selected_model': selected_model,
            'baseline': manager.baseline,
            'comparison': comparison,
            'all_models': available_models
        }
        
        st.json(export_data)
        
        # Bouton de t√©l√©chargement CSV
        csv_data = comparison_df.to_csv(index=False)
        st.download_button(
            label="üìä T√©l√©charger le tableau en CSV",
            data=csv_data,
            file_name=f"model_comparison_{selected_model['model_id']}.csv",
            mime="text/csv"
        )

if __name__ == "__main__":
    main()